{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "Design an ANN ( Feed Forward ANN ) to identify handwritten digits ( MNIST dataset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "#Flatten image\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#Normalize pixel data\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initially we consider batch size of 128 and 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model: \n",
    "- 1 hidden layer with 512 nodes with relu activation\n",
    "- Output layer uses softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise model\n",
    "model = Sequential()\n",
    "#Add input and hidden layer\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#Add output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling model: Using categorical crossentropy loss function and sgd optimiser \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, nesterov=False),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s - loss: 1.1048 - acc: 0.7543 - val_loss: 0.6035 - val_acc: 0.8629\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.5241 - acc: 0.8704 - val_loss: 0.4358 - val_acc: 0.8891\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.4215 - acc: 0.8890 - val_loss: 0.3754 - val_acc: 0.9002\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.3750 - acc: 0.8980 - val_loss: 0.3420 - val_acc: 0.9071\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.3464 - acc: 0.9037 - val_loss: 0.3205 - val_acc: 0.9133\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.3260 - acc: 0.9087 - val_loss: 0.3036 - val_acc: 0.9170\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.3099 - acc: 0.9133 - val_loss: 0.2910 - val_acc: 0.9192\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2966 - acc: 0.9168 - val_loss: 0.2802 - val_acc: 0.9225\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2854 - acc: 0.9200 - val_loss: 0.2705 - val_acc: 0.9243\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.2755 - acc: 0.9222 - val_loss: 0.2613 - val_acc: 0.9258\n",
      "Test loss: 0.261310408142\n",
      "Test accuracy: 0.9258\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "#Evaluating model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model:\n",
    "- 2 hidden layer with 512 nodes with relu activation\n",
    "- Output layer uses softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise model\n",
    "model = Sequential()\n",
    "#Add input and 1st hidden layer\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#Add 2nd hidden layer\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#Add output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling model: Using categorical crossentropy loss function and sgd optimiser \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, nesterov=False),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s - loss: 1.1186 - acc: 0.7496 - val_loss: 0.5250 - val_acc: 0.8752\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.4505 - acc: 0.8812 - val_loss: 0.3695 - val_acc: 0.9013\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.3586 - acc: 0.8999 - val_loss: 0.3173 - val_acc: 0.9099\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.3176 - acc: 0.9099 - val_loss: 0.2878 - val_acc: 0.9205\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.2915 - acc: 0.9174 - val_loss: 0.2674 - val_acc: 0.9245\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.2717 - acc: 0.9230 - val_loss: 0.2511 - val_acc: 0.9298\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.2560 - acc: 0.9272 - val_loss: 0.2423 - val_acc: 0.9333\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.2425 - acc: 0.9312 - val_loss: 0.2288 - val_acc: 0.9351.93\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.2304 - acc: 0.9348 - val_loss: 0.2202 - val_acc: 0.9377\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.2200 - acc: 0.9374 - val_loss: 0.2120 - val_acc: 0.9382\n",
      "Test loss: 0.211956237541\n",
      "Test accuracy: 0.9382\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "#Evaluating model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Model:\n",
    "- 1 hidden layer with 512 nodes with sigmoid activation\n",
    "- Output layer uses softmax activation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise model\n",
    "model = Sequential()\n",
    "#Add input and hidden layer\n",
    "model.add(Dense(512, activation='sigmoid', input_shape=(784,)))\n",
    "#Add output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling model: Using categorical crossentropy loss function and sgd optimiser \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, nesterov=False),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 9s - loss: 1.9563 - acc: 0.5307 - val_loss: 1.6145 - val_acc: 0.7358\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 9s - loss: 1.3817 - acc: 0.7611 - val_loss: 1.1564 - val_acc: 0.7999\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 8s - loss: 1.0336 - acc: 0.8090 - val_loss: 0.8975 - val_acc: 0.8272\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 9s - loss: 0.8359 - acc: 0.8318 - val_loss: 0.7481 - val_acc: 0.8460\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 9s - loss: 0.7166 - acc: 0.8451 - val_loss: 0.6535 - val_acc: 0.8581\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 9s - loss: 0.6384 - acc: 0.8552 - val_loss: 0.5888 - val_acc: 0.8672\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 9s - loss: 0.5836 - acc: 0.8626 - val_loss: 0.5416 - val_acc: 0.8709\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 9s - loss: 0.5430 - acc: 0.8677 - val_loss: 0.5064 - val_acc: 0.8762\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 9s - loss: 0.5119 - acc: 0.8723 - val_loss: 0.4795 - val_acc: 0.8802\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 9s - loss: 0.4872 - acc: 0.8764 - val_loss: 0.4573 - val_acc: 0.8835\n",
      "Test loss: 0.45727905581\n",
      "Test accuracy: 0.8835\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Model:\n",
    "- 2 hidden layer with 512 nodes with sigmoid activation\n",
    "- Output layer uses softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise model\n",
    "model = Sequential()\n",
    "#Add input and 1st hidden layer\n",
    "model.add(Dense(512, activation='sigmoid', input_shape=(784,)))\n",
    "#Add 2nd hidden layer\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "#Add output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling model: Using categorical crossentropy loss function and sgd optimiser\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, nesterov=False),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 17s - loss: 2.2694 - acc: 0.2013 - val_loss: 2.2288 - val_acc: 0.3223\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 17s - loss: 2.1863 - acc: 0.3811 - val_loss: 2.1337 - val_acc: 0.4811\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 16s - loss: 2.0766 - acc: 0.5276 - val_loss: 1.9972 - val_acc: 0.6421\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 16s - loss: 1.9150 - acc: 0.6062 - val_loss: 1.8022 - val_acc: 0.6409\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 16s - loss: 1.6927 - acc: 0.6556 - val_loss: 1.5525 - val_acc: 0.7362\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 16s - loss: 1.4434 - acc: 0.6970 - val_loss: 1.3060 - val_acc: 0.7541\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 12s - loss: 1.2203 - acc: 0.7353 - val_loss: 1.1078 - val_acc: 0.7609\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s - loss: 1.0482 - acc: 0.7665 - val_loss: 0.9614 - val_acc: 0.7894\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.9211 - acc: 0.7873 - val_loss: 0.8521 - val_acc: 0.7934\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s - loss: 0.8258 - acc: 0.8042 - val_loss: 0.7688 - val_acc: 0.8144\n",
      "Test loss: 0.768779523563\n",
      "Test accuracy: 0.8144\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "#Evaluating model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth Model:\n",
    "- 1 hidden layer with 512 nodes with tanh activation\n",
    "- Output layer uses softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise model\n",
    "model = Sequential()\n",
    "#Add input and 1st hidden layer\n",
    "model.add(Dense(512, activation='tanh', input_shape=(784,)))\n",
    "#Add output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling model: Using categorical crossentropy loss function and sgd optimiser\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, nesterov=False),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.9374 - acc: 0.7750 - val_loss: 0.5455 - val_acc: 0.8667\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.4975 - acc: 0.8704 - val_loss: 0.4270 - val_acc: 0.8886\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.4221 - acc: 0.8851 - val_loss: 0.3798 - val_acc: 0.8962\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.3860 - acc: 0.8928 - val_loss: 0.3543 - val_acc: 0.9015\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.3638 - acc: 0.8981 - val_loss: 0.3375 - val_acc: 0.9061\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.3482 - acc: 0.9016 - val_loss: 0.3254 - val_acc: 0.9094\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.3364 - acc: 0.9051 - val_loss: 0.3160 - val_acc: 0.9114\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.3270 - acc: 0.9072 - val_loss: 0.3101 - val_acc: 0.9121\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.3193 - acc: 0.9094 - val_loss: 0.3039 - val_acc: 0.9149\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.3127 - acc: 0.9113 - val_loss: 0.2979 - val_acc: 0.9169\n",
      "Test loss: 0.297877052099\n",
      "Test accuracy: 0.9169\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "#Evaluating model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sixth Model:\n",
    "- 2 hidden layer with 512 nodes with tanh activation\n",
    "- Output layer uses softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise model\n",
    "model = Sequential()\n",
    "#Add input and 1st hidden layer\n",
    "model.add(Dense(512, activation='tanh', input_shape=(784,)))\n",
    "#Add 2nd hidden layer\n",
    "model.add(Dense(512, activation='tanh'))\n",
    "#Add output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling model: Using categorical crossentropy loss function and sgd optimiser\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, nesterov=False),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.8065 - acc: 0.8069 - val_loss: 0.4555 - val_acc: 0.8831\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.4232 - acc: 0.8857 - val_loss: 0.3666 - val_acc: 0.9003\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.3659 - acc: 0.8980 - val_loss: 0.3330 - val_acc: 0.9076\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.3379 - acc: 0.9052 - val_loss: 0.3152 - val_acc: 0.9121\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.3206 - acc: 0.9090 - val_loss: 0.3008 - val_acc: 0.9156\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.3076 - acc: 0.9123 - val_loss: 0.2908 - val_acc: 0.9172\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.2973 - acc: 0.9154 - val_loss: 0.2834 - val_acc: 0.9201\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s - loss: 0.2887 - acc: 0.9179 - val_loss: 0.2774 - val_acc: 0.9211\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.2812 - acc: 0.9204 - val_loss: 0.2704 - val_acc: 0.9227\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 17s - loss: 0.2746 - acc: 0.9222 - val_loss: 0.2651 - val_acc: 0.9238\n",
      "Test loss: 0.265148718074\n",
      "Test accuracy: 0.9238\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "#Evaluating model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the best model: 2 hidden layers with relu and output layer with softmax activation and increasing the number of epochs to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise model\n",
    "model = Sequential()\n",
    "#Add input and 1st hidden layer\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#Add 2nd hidden layer\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#Add output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling model: Using categorical crossentropy loss function and sgd optimiser\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, nesterov=False),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 12s - loss: 1.0846 - acc: 0.7670 - val_loss: 0.5156 - val_acc: 0.8759\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 11s - loss: 0.4443 - acc: 0.8856 - val_loss: 0.3670 - val_acc: 0.9001\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.3553 - acc: 0.9027 - val_loss: 0.3179 - val_acc: 0.9090\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.3155 - acc: 0.9121 - val_loss: 0.2871 - val_acc: 0.9193\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.2893 - acc: 0.9182 - val_loss: 0.2685 - val_acc: 0.9253\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 11s - loss: 0.2700 - acc: 0.9240 - val_loss: 0.2528 - val_acc: 0.9281\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 11s - loss: 0.2544 - acc: 0.9283 - val_loss: 0.2416 - val_acc: 0.9315.927 - ETA: 1\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.2411 - acc: 0.9320 - val_loss: 0.2283 - val_acc: 0.9350\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.2294 - acc: 0.9355 - val_loss: 0.2201 - val_acc: 0.9382\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 9s - loss: 0.2188 - acc: 0.9387 - val_loss: 0.2100 - val_acc: 0.9403\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 9s - loss: 0.2094 - acc: 0.9413 - val_loss: 0.2020 - val_acc: 0.9430\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.2009 - acc: 0.9435 - val_loss: 0.1953 - val_acc: 0.9430\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.1932 - acc: 0.9457 - val_loss: 0.1882 - val_acc: 0.9456\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.1857 - acc: 0.9482 - val_loss: 0.1829 - val_acc: 0.9483\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 12s - loss: 0.1791 - acc: 0.9496 - val_loss: 0.1781 - val_acc: 0.9483\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.1727 - acc: 0.9517 - val_loss: 0.1719 - val_acc: 0.9519\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.1667 - acc: 0.9529 - val_loss: 0.1666 - val_acc: 0.9526\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.1612 - acc: 0.9541 - val_loss: 0.1609 - val_acc: 0.9539\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.1561 - acc: 0.9556 - val_loss: 0.1588 - val_acc: 0.9539\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.1511 - acc: 0.9573 - val_loss: 0.1543 - val_acc: 0.9548\n",
      "Test loss: 0.154266804539\n",
      "Test accuracy: 0.9548\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "#Evaluating model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "Increasing the number of epochs gave us better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise model\n",
    "model = Sequential()\n",
    "#Add input and 1st hidden layer\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#Add 2nd hidden layer\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#Add output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling model: Using categorical crossentropy loss function and sgd optimiser\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, nesterov=False),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 9s - loss: 1.5178 - acc: 0.6753 - val_loss: 0.8609 - val_acc: 0.8378\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 9s - loss: 0.6703 - acc: 0.8524 - val_loss: 0.5153 - val_acc: 0.8775\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 9s - loss: 0.4801 - acc: 0.8778 - val_loss: 0.4172 - val_acc: 0.8947\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 9s - loss: 0.4086 - acc: 0.8905 - val_loss: 0.3680 - val_acc: 0.9037\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s - loss: 0.3696 - acc: 0.8980 - val_loss: 0.3390 - val_acc: 0.9103\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s - loss: 0.3438 - acc: 0.9041 - val_loss: 0.3192 - val_acc: 0.9142\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.3250 - acc: 0.9092 - val_loss: 0.3043 - val_acc: 0.9167\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.3102 - acc: 0.9128 - val_loss: 0.2917 - val_acc: 0.9190\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.2975 - acc: 0.9159 - val_loss: 0.2818 - val_acc: 0.9215\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s - loss: 0.2870 - acc: 0.9184 - val_loss: 0.2726 - val_acc: 0.9258\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.2774 - acc: 0.9211 - val_loss: 0.2648 - val_acc: 0.9249\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.2688 - acc: 0.9238 - val_loss: 0.2585 - val_acc: 0.9287\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.2613 - acc: 0.9259 - val_loss: 0.2519 - val_acc: 0.9301\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.2541 - acc: 0.9275 - val_loss: 0.2457 - val_acc: 0.9306\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.2474 - acc: 0.9294 - val_loss: 0.2399 - val_acc: 0.9324\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.2412 - acc: 0.9316 - val_loss: 0.2356 - val_acc: 0.9334\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.2354 - acc: 0.9327 - val_loss: 0.2294 - val_acc: 0.9356\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.2298 - acc: 0.9344 - val_loss: 0.2253 - val_acc: 0.9359\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.2245 - acc: 0.9364 - val_loss: 0.2196 - val_acc: 0.9377\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.2194 - acc: 0.9382 - val_loss: 0.2155 - val_acc: 0.9390\n",
      "Test loss: 0.215541495086\n",
      "Test accuracy: 0.939\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "#Evaluating model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "Increasing batch size didnt increase accuracy, it decreased infact so we'll be sticking to the old batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying optimiser sgd with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise model\n",
    "model = Sequential()\n",
    "#Add input and 1st hidden layer\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#Add 2nd hidden layer\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#Add output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print model information\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling model: Using categorical crossentropy loss function and sgd optimiser with momentum\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=False),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 5s - loss: 0.4347 - acc: 0.8792 - val_loss: 0.2182 - val_acc: 0.9381\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s - loss: 0.1952 - acc: 0.9437 - val_loss: 0.1587 - val_acc: 0.9531\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s - loss: 0.1428 - acc: 0.9590 - val_loss: 0.1479 - val_acc: 0.9551\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s - loss: 0.1131 - acc: 0.9677 - val_loss: 0.1045 - val_acc: 0.9685\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.0914 - acc: 0.9738 - val_loss: 0.0974 - val_acc: 0.9698\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.0764 - acc: 0.9780 - val_loss: 0.0880 - val_acc: 0.9720\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.0644 - acc: 0.9812 - val_loss: 0.0843 - val_acc: 0.9739\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 6s - loss: 0.0559 - acc: 0.9847 - val_loss: 0.0743 - val_acc: 0.9763\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 6s - loss: 0.0478 - acc: 0.9867 - val_loss: 0.0723 - val_acc: 0.9769\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s - loss: 0.0418 - acc: 0.9886 - val_loss: 0.0710 - val_acc: 0.9791\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 6s - loss: 0.0363 - acc: 0.9905 - val_loss: 0.0692 - val_acc: 0.9783\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.0320 - acc: 0.9914 - val_loss: 0.0726 - val_acc: 0.9773\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s - loss: 0.0281 - acc: 0.9931 - val_loss: 0.0669 - val_acc: 0.9784\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.0237 - acc: 0.9944 - val_loss: 0.0658 - val_acc: 0.9793\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0652 - val_acc: 0.9793\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.0184 - acc: 0.9962 - val_loss: 0.0629 - val_acc: 0.9802\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s - loss: 0.0162 - acc: 0.9968 - val_loss: 0.0632 - val_acc: 0.9806\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.0138 - acc: 0.9979 - val_loss: 0.0637 - val_acc: 0.9805\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.0124 - acc: 0.9981 - val_loss: 0.0629 - val_acc: 0.9802\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.0110 - acc: 0.9986 - val_loss: 0.0620 - val_acc: 0.9816\n",
      "Test loss: 0.0619707350643\n",
      "Test accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "#Evaluating model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out adagrad optimser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise model\n",
    "model = Sequential()\n",
    "#Add input and 1st hidden layer\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#Add 2nd hidden layer\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#Add output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling model: Using categorical crossentropy loss function and adagrad optimiser\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adagrad',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 17s - loss: 0.2183 - acc: 0.9373 - val_loss: 0.0926 - val_acc: 0.9701\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 18s - loss: 0.0751 - acc: 0.9777 - val_loss: 0.0778 - val_acc: 0.9736\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 19s - loss: 0.0496 - acc: 0.9855 - val_loss: 0.0695 - val_acc: 0.9784\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 18s - loss: 0.0350 - acc: 0.9904 - val_loss: 0.0611 - val_acc: 0.9802\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 17s - loss: 0.0254 - acc: 0.9936 - val_loss: 0.0595 - val_acc: 0.9821\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 17s - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0577 - val_acc: 0.9820\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 12s - loss: 0.0141 - acc: 0.9972 - val_loss: 0.0639 - val_acc: 0.9810\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 12s - loss: 0.0109 - acc: 0.9981 - val_loss: 0.0588 - val_acc: 0.9824\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 13s - loss: 0.0083 - acc: 0.9989 - val_loss: 0.0582 - val_acc: 0.9830\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 16s - loss: 0.0064 - acc: 0.9993 - val_loss: 0.0593 - val_acc: 0.9821\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 17s - loss: 0.0052 - acc: 0.9995 - val_loss: 0.0593 - val_acc: 0.9836\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 18s - loss: 0.0042 - acc: 0.9997 - val_loss: 0.0609 - val_acc: 0.9830\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 18s - loss: 0.0035 - acc: 0.9998 - val_loss: 0.0601 - val_acc: 0.9829\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 16s - loss: 0.0029 - acc: 0.9999 - val_loss: 0.0604 - val_acc: 0.9839\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 18s - loss: 0.0025 - acc: 0.9999 - val_loss: 0.0612 - val_acc: 0.9833\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 19s - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0612 - val_acc: 0.9835\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 18s - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9834\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 17s - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9839\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 18s - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0638 - val_acc: 0.9829\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 17s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0644 - val_acc: 0.9835\n",
      "Test loss: 0.0644385387423\n",
      "Test accuracy: 0.9835\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "#Evaluating model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "Adagrad optimiser gives us the best accuracy of 98.35%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting loss and accuracy of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to plot loss and accuracy\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4IAAAFNCAYAAABVKNEpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnmf0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff97f4cd7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "Best Results were obtained with\n",
    "2 hidden layers of 512 nodes each with 'relu' activation and an output layer with 'softmax' activation\n",
    "The Adagrad optimiser gave us the best result with an accuracy of 98.35%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
